{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='You are a helpful assistant that translates English to French' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content=\"what's 5 + 2\" additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='5 + 2 is 7' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='I love programming.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "template: ChatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"history\", n_messages=2),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "value: ChatPromptValue = template.format_prompt(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"French\",\n",
    "    history=[(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
    "    text=\"I love programming.\",\n",
    ")\n",
    "\n",
    "messages: list[BaseMessage] = value.to_messages()\n",
    "for m in messages:\n",
    "    print(type(m))\n",
    "    print(m.get_lc_namespace())\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"hello\")\n",
    "print(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM\n",
    "from transformers.models.qwen2.tokenization_qwen2_fast import (\n",
    "    Qwen2TokenizerFast,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"/root/xiatian/models/Qwen2.5-7B-Instruct\"\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "tokenizer: Qwen2TokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda:7\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题在于“珍于”应为“臻于”，“自然”应为“文化遗产”。\n",
      "\n",
      "修改后的句子：\n",
      "中国城墙的各种物质形态经过数百年的发展、传承和演进，到明清时代已臻于高度成熟，故宫城墙既是明清官式城墙类建筑的典范。故宫城墙始建于明永乐四年（1406年），建成于永乐十八年（1420年），是我国现存规模最大、保存最完整的皇家宫殿城墙，在中国筑城史上占据极其重要的地位。故宫城墙已经列入世界文化遗产名录。\n"
     ]
    }
   ],
   "source": [
    "role = \"你是校对助手，负责解决文字中存在的错别字、文字差错、逻辑错误、数字计算错误以及不符合常识的错误，并给出正确答案。请先进行仔细分析，给出推理步骤，遇到数字，则调用计算器进行计算以便验证。分析完毕后，在最后列出存在的问题，然后在最后一行输出修改后的完整句子，如果没有错误，则在最后一行输出：句子无错误\"\n",
    "role = \"你是校对助手，负责解决文字中存在的错别字、文字差错、逻辑错误、数字计算错误以及不符合常识的错误，并给出正确答案。请先进行仔细分析，给出推理步骤，遇到数字，则调用计算器进行计算以便验证。在修改结果时，优先选择拼音接近或者字形接近的字词。分析完毕后，列出存在的问题。最后，如果句子没有问题，则输出：句子无错误，否则输出修改后的完整句子。\"\n",
    "role = \"你是校对助手，解决文字中存在的错别字词的误用问题，修改时选择拼音或字形接近的字词，思考后列出问题，再输出修改后句子。\"\n",
    "prompt = \"他一年用掉了三万多把扫帚打扫卫生。\"\n",
    "prompt = \"据估计，2019年北京市垃圾处理总吨数将突破1000万吨，处理1吨垃圾的费用约为300元/吨，处理这些垃圾的总费用就高达3亿元。\"\n",
    "prompt = \"青铜峡河段长约8.2公里，宽50100米，两侧的崖壁高30米以上，具有典型的北方粗犷雄渾特色，被称为“黄河上游第一峡谷”。\"\n",
    "prompt = \"\"\"黄河全长约5644公里，流域面积约75万公里。\"\"\"\n",
    "prompt = \"\"\"5.下列四组词语中，没有错别字的一组是：\n",
    "A.羁绊    振振有词 执著.     B.  老奸巨猾  惟美主义 恰如其分\n",
    "C.勘误  绝不放弃  消声匿迹    D.流蹿  性格不和  进退维谷\n",
    "\"\"\"\n",
    "prompt = \"\"\"9.下列典籍中，属于“四书五经”中“四书”的是：\n",
    "A.《论语》\n",
    "B.《中庸》\n",
    "C.《周易》\n",
    "D.《孟子》\"\"\"\n",
    "prompt = \"纠正错别字：既要坚持‘内容为王’，也要加强‘渠道致胜’\"\n",
    "# prompt = \"惟美主义\"\n",
    "# prompt = \"习平书记出席了会议。\"\n",
    "prompt = \"中国城墙的各种物质形态经过数百年的发展、传承和演进，到明清时代已珍于高度成熟，故宫城墙既是明清官式城墙类建筑的典范。故宫城墙始建于明永乐四年（1406年），建成于永乐十八年（1420年），是我国现存规模最大、保存最完整的皇家宫殿城墙，在中国筑城史上占据及其重要的地位。故宫城墙已经列入世界自然遗产名录。\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": role,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text: str = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer(\n",
    "    [text], return_tensors=\"pt\", max_length=512, truncation=True\n",
    ")\n",
    "model_inputs = model_inputs.to(model.device)\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "config = GenerationConfig()\n",
    "config.temperature = 0\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs, generation_config=config, max_new_tokens=512\n",
    ")\n",
    "\n",
    "# print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])\n",
    "\n",
    "# 生成的id删除前面和输入相同的id\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True\n",
    ")\n",
    "print(prompt)\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowpath-0Zuin7zc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
