{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='You are a helpful assistant that translates English to French' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content=\"what's 5 + 2\" additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='5 + 2 is 7' additional_kwargs={} response_metadata={}\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "['langchain', 'schema', 'messages']\n",
      "content='I love programming.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "template: ChatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"history\", n_messages=2),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "value: ChatPromptValue = template.format_prompt(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"French\",\n",
    "    history=[(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
    "    text=\"I love programming.\",\n",
    ")\n",
    "\n",
    "messages: list[BaseMessage] = value.to_messages()\n",
    "for m in messages:\n",
    "    print(type(m))\n",
    "    print(m.get_lc_namespace())\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "password = getpass.getpass(\"hello\")\n",
    "print(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/knowpath-0Zuin7zc-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM\n",
    "from transformers.models.qwen2.tokenization_qwen2_fast import (\n",
    "    Qwen2TokenizerFast,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"/root/xiatian/models/Qwen2.5-7B-Instruct\"\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "tokenizer: Qwen2TokenizerFast = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是知路助手，负责解决文字中存在的逻辑错误、数字计算错误以及不符合常识的错误，并给出正确答案。请先进行仔细分析，给出推理步骤，遇到数字，则调用计算器进行计算以便验证。分析完毕后，在最后一行输出修改后的结果，如果句子没有问题，则在最后一行输出：句子无错误<|im_end|>\n",
      "<|im_start|>user\n",
      "据估计，2019年北京市垃圾处理总吨数将突破1000万吨，处理1吨垃圾的费用约为300元/吨，处理这些垃圾的总费用就高达3亿元。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "首先，我们来验证一下处理1000万吨垃圾的总费用是否为3亿元。\n",
      "\n",
      "1. 1000万吨垃圾乘以每吨300元的处理费用：\n",
      "\\[ 1000 \\times 10^6 \\text{ 吨} \\times 300 \\text{ 元/吨} = 300,000,000,000 \\text{ 元} \\]\n",
      "\n",
      "2. 将上述金额转换为亿元：\n",
      "\\[ 300,000,000,000 \\text{ 元} = 30,000 \\text{ 亿元} \\]\n",
      "\n",
      "显然，处理1000万吨垃圾的总费用应为30,000亿元而不是3亿元。因此，原句中的费用数值存在错误。\n",
      "\n",
      "修改后的结果：据估计，2019年北京市垃圾处理总吨数将突破1000万吨，处理1吨垃圾的费用约为300元/吨，处理这些垃圾的总费用就高达30,000亿元。<|im_end|>\n",
      "首先，我们来验证一下处理1000万吨垃圾的总费用是否为3亿元。\n",
      "\n",
      "1. 1000万吨垃圾乘以每吨300元的处理费用：\n",
      "\\[ 1000 \\times 10^6 \\text{ 吨} \\times 300 \\text{ 元/吨} = 300,000,000,000 \\text{ 元} \\]\n",
      "\n",
      "2. 将上述金额转换为亿元：\n",
      "\\[ 300,000,000,000 \\text{ 元} = 30,000 \\text{ 亿元} \\]\n",
      "\n",
      "显然，处理1000万吨垃圾的总费用应为30,000亿元而不是3亿元。因此，原句中的费用数值存在错误。\n",
      "\n",
      "修改后的结果：据估计，2019年北京市垃圾处理总吨数将突破1000万吨，处理1吨垃圾的费用约为300元/吨，处理这些垃圾的总费用就高达30,000亿元。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"他一年用掉了三万多把扫帚打扫卫生。\"\n",
    "prompt = \"据估计，2019年北京市垃圾处理总吨数将突破1000万吨，处理1吨垃圾的费用约为300元/吨，处理这些垃圾的总费用就高达3亿元。\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是知路助手，负责解决文字中存在的逻辑错误、数字计算错误以及不符合常识的错误，并给出正确答案。请先进行仔细分析，给出推理步骤，遇到数字，则调用计算器进行计算以便验证。分析完毕后，在最后一行输出修改后的结果，如果句子没有问题，则在最后一行输出：句子无错误\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "text: str = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer(\n",
    "    [text], return_tensors=\"pt\", max_length=512, truncation=True\n",
    ")\n",
    "model_inputs = model_inputs.to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])\n",
    "\n",
    "# 生成的id删除前面和输入相同的id\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(\n",
    "    generated_ids, skip_special_tokens=False\n",
    ")\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这句话中的逻辑有些不合理，因为一把扫帚不可能在一年内被用掉。如果你的意思是他一年用了大量的扫帚，可以这样表述：\n",
      "\n",
      "- 他一年用掉了三千多把扫帚打扫卫生。\n",
      "- 他一年更换了三万多把扫帚来打扫卫生。\n",
      "\n",
      "具体取决于你想要表达的实际情况。\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowpath-0Zuin7zc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
